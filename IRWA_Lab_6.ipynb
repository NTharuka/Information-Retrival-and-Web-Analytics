{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "1. Implement a program to test if a given page is found or not on the server."
      ],
      "metadata": {
        "id": "TFv4EJSMhNqw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#HTTP Error\n",
        "\n",
        "import urllib.request\n",
        "import urllib.parse\n",
        "\n",
        "#trying to read the URL\n",
        "try:\n",
        "  x = urllib.request.urlopen('https://www.googlecccc.com/')\n",
        "  #print(x.read())\n",
        "\n",
        "# Catching the exception generated\n",
        "except Exception as e :\n",
        "  print(str(e))\n"
      ],
      "metadata": {
        "id": "lvNlcKZ-ne6W",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2286972a-418d-41bb-846e-234120075078"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<urlopen error [Errno -2] Name or service not known>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. Python implementation to show the content of robot.txt for sliit.lk. "
      ],
      "metadata": {
        "id": "YX0wVdlUhadW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "response = requests.get(\"https://www.sliit.lk/robots.txt\")\n",
        "test = response.text\n",
        "print(\"robots.txt for https://www.sliit.lk/\")\n",
        "print(\"====================================================\")\n",
        "print(test)"
      ],
      "metadata": {
        "id": "l6ZWmVRfhbi_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "3. Python implementation to print first <h1> tag from\n",
        "https://en.wikipedia.org/wiki/Main_Page \n",
        "\n",
        "*   List item\n",
        "*   List item\n",
        "\n"
      ],
      "metadata": {
        "id": "T5O6MhaAjwlm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from urllib.request import urlopen\n",
        "from bs4 import BeautifulSoup\n",
        "html = urlopen('https://en.wikipedia.org/wiki/Main_Page')\n",
        "bs = BeautifulSoup(html)  #to scrap data from source code\n",
        "titles = bs.find(['h1'])\n",
        "print('List the first header tag :', *titles, sep='\\n\\n')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QJqe3hoQmedT",
        "outputId": "21708194-511e-4158-8946-9a3e58b60e5d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "List the first header tag :\n",
            "\n",
            "<span class=\"mw-page-title-main\">Main Page</span>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from urllib.request import urlopen\n",
        "from bs4 import BeautifulSoup\n",
        "html = urlopen('https://en.wikipedia.org/wiki/Main_Page')\n",
        "bs = BeautifulSoup(html)  #to scrap data from source code\n",
        "titles = bs.find_all(['h1','h2','h3'])\n",
        "print('List the first header tag :', *titles, sep='\\n\\n')"
      ],
      "metadata": {
        "id": "6-tFkUPVpxaD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "5. How to display all the image links from \n",
        "https://www.ibm.com/lk-en. "
      ],
      "metadata": {
        "id": "RG9sPiEouILg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from urllib.request import urlopen\n",
        "from bs4 import BeautifulSoup\n",
        "import re\n",
        "\n",
        "html = urlopen('https://www.ibm.com/lk-en')\n",
        "bs = BeautifulSoup(html, 'html.parser')\n",
        "images = bs.find_all('img')\n",
        "for image in images:\n",
        "  print(image['src']+'\\n')"
      ],
      "metadata": {
        "id": "KW2lX31Mshdx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "6. Python implementation to retrieve all the links in a particular URL"
      ],
      "metadata": {
        "id": "pPHgH0C-mixi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from urllib.request import urlopen\n",
        "from bs4 import BeautifulSoup\n",
        "\n",
        "html = urlopen('https://www.sliit.lk/')\n",
        "bs = BeautifulSoup(html, 'lxml')\n",
        "links = bs.find_all('a')\n",
        "\n",
        "for link in links:\n",
        "  print(link.get('href'))"
      ],
      "metadata": {
        "id": "rYTMYGKLmj3x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "7. Python implementation to display section that contain a specified string in a given web\n",
        "page."
      ],
      "metadata": {
        "id": "AfnFb48kz9PU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import re\n",
        "\n",
        "url=\"https://en.wikipedia.org/wiki/Main_Page\"\n",
        "req=requests.get(url)\n",
        "bs = BeautifulSoup(req.text, 'lxml')\n",
        "str1=bs.find_all(string=re.compile('a government'))\n",
        "for txt in str1:\n",
        "  print(txt)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t-bpO_yM0BTj",
        "outputId": "b1fa7f5c-992b-4be9-d253-9385a4bcc434"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "a government crisis\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "8. Python implementation to obtain html code of parent content of a particular html tag."
      ],
      "metadata": {
        "id": "j9npVstZ-O_-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "url= \"https://www.sliit.lk/\"\n",
        "req=requests.get(url)\n",
        "bs = BeautifulSoup(req.text, 'lxml')\n",
        "print(bs.title.text)\n",
        "print(bs.title.parent)"
      ],
      "metadata": {
        "id": "02__5u_7-QAA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "9. Python implementation to obtain html code of child nodes inside a particular html tag"
      ],
      "metadata": {
        "id": "Kn3QcsfACVtd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "\n",
        "url = \"https://www.sliit.lk/\"\n",
        "req=requests.get(url)\n",
        "bs = BeautifulSoup(req.text, 'lxml')\n",
        "print(*bs.header.parent)"
      ],
      "metadata": {
        "id": "raz-CncOCPof"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "10. Build a web scrapper using BeautifulSoup to extract (scrape) movie data(url, name,\n",
        "summary) from https://www.imdb.com/chart/top/ in to a excel file."
      ],
      "metadata": {
        "id": "QlS8055nj3rK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "import lxml\n",
        "from bs4 import BeautifulSoup\n",
        "from xlwt import *\n",
        "\n",
        "workbook = Workbook(encoding = 'utf-8')\n",
        "table = workbook.add_sheet('data')\n",
        "#create the header of each column in the first row.\n",
        "table.write(0,0,'Number')\n",
        "table.write(0,1,'movie_url')\n",
        "table.write(0,2,'movie_name')\n",
        "table.write(0,3,'movie_introduction')\n",
        "#Write the crawled data into Excel separately from the second row.\n",
        "line = 1\n",
        "\n",
        "url = \"https://www.rottentomatoes.com/top/bestofrt/\"\n",
        "headers = {\n",
        "    'User-Agent': 'Mozilla/5.0 (Windows NT 6.1; wow64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/63.0.3239.132 Safari/537.36 QIHU 360SE'\n",
        "}\n",
        "\n",
        "\n",
        "f = requests.get(url, headers = headers)\n",
        "movies_lst = []\n",
        "soup = BeautifulSoup(f.content, 'lxml')\n",
        "movies = soup.find('table', {'class':'table'}).find_all('a')\n",
        "num = 0\n",
        "for anchor in movies:\n",
        "  urls = 'https://www.rottentomatoes.com' + anchor['href']\n",
        "  movies_lst.append(urls)\n",
        "  num += 1\n",
        "  movie_url = urls\n",
        "  movie_f = requests.get(movie_url, headers = headers)\n",
        "  movie_soup\n"
      ],
      "metadata": {
        "id": "vwMea8-Nj8vK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from urllib import request\n",
        "import requests\n",
        "import lxml\n",
        "from bs4 import BeautifulSoup\n",
        "from xlwt import *\n",
        "\n",
        "workbook = Workbook(encoding = 'utf-8')\n",
        "table = workbook.add_sheet('data')\n",
        "\n",
        "table.write(0, 0, 'Number')\n",
        "table.write(0, 1, 'movie_url')\n",
        "table.write(0, 2, 'movie_name')\n",
        "table.write(0, 3, 'movie_introduction')\n",
        "line = 1\n",
        "\n",
        "url = \" https://www.imdb.com/chart/top/\"\n",
        "header = {\n",
        "    'User-Agent': 'Mozilla'\n",
        "}\n",
        "\n",
        "f = request.get(url, headers = headers)\n",
        "movies_lst = []\n",
        "soup = BeautifulSoup(f.content, 'lxml')\n",
        "movies = soup.find('table',{'class':'chart full-width'}).find_all('td', {'class':'titleColumn'})\n",
        "links = [movie.find_all('a') for movie in movies]\n",
        "\n",
        "#print(len(links))\n",
        "\n",
        "num = 0\n",
        "for anchor in links:\n",
        "  urls = 'https://www.imdb.com' + anchor[0]['href']\n",
        "  movies_lst.append(urls)\n",
        "  num += 1\n",
        "  movie_url = urls\n",
        "  movie_f = requests.get(movie_url,headers = headers)\n",
        "  movie_soup = BeautifulSoup(movie_f.content, 'lxml')\n",
        "  movie_content = movie_soup.find('span',{'data-testid': 'plot-xl'})\n",
        "  movie_name = movie_soup.find('div, {'class':'TileBlock_TitleContainer-sc-1nlhx7j-1 jxsVNt'}).find_all('h1')\n",
        "  print(num, urls, '\\n', 'Movie:' +movie_name[0].string.strip())\n",
        "  print(movie_content.string)\n",
        "  print('Movie info:' + movie_content.string)\n",
        "  table.write(line, 0,num)\n",
        "  table.write(line, 1,urls)\n",
        "  table.write(line, 2,movie_name[0].string.strip())\n",
        "  table.write(line, 3,movie_content.string)\n",
        "  line += 1\n",
        "workbook.save('dissanayaka_movies_top100.xls')"
      ],
      "metadata": {
        "id": "QTMJfjNk1kh1"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}